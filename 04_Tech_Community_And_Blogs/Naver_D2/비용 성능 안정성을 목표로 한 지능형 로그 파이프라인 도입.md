# 비용, 성능, 안정성을 목표로 한 지능형 로그 파이프라인 도입

**출처:** [Naver_D2](https://d2.naver.com/helloworld/0004394)

## 요약
Logiss는 AIDA(Advanced Interface for Data & AI)라는 네이버 사내 통합 데이터 플랫폼의 일부로, 로그 수집과 실시간 검색을 통해 문제를 추적하고 데이터를 분석하도록 지원합니다. 또한 Cuve와 CQuery라는 대용량 데이터 저장소 및 분석 설루션과 데이터를 연동합니다.

이 글에서는 로그 파이프라인을 운영하면서 겪은 문제점과 지능형 로그 파이프라인을 도입해 이를 해결한 과정을 공유합니다.

관련 발표는 **팀네이버 컨퍼런스 DAN25** '[하루 수백억 건을 처리하는 똑똑한 로그 파이프라인 만들기: 비용·성능·안정성 삼박자](https://dan.naver.com/25/sessions/693)'에서도 살펴보실 수 있습니다.

로그 파이프라인의 역할
------------

로그 파이프라인을 설명하기 전에 먼저 AIDA의 주요 컴포넌트 구성을 간단히 알아보겠습니다.

![](https://d2.naver.com/content/images/2025/12/2.png)

AIDA의 주요 컴포넌트 구성

* **AIDA Project**: 각 컴포넌트의 리소스 사용 관리, 데이터 권한 관리 담당
* **Cuve**: HBase, Kafka 기반의 검색에 필요한 모든 문서를 중심으로 데이터 저장 및 유통
  + **Hades**: 검색 결과에서 문서 노출 여부 제어
  + **Deathnote**: 서버 실시간 문서 삭제
* **C3**: Apache Hadoop(+보안 패치, C3 패치) 기반의 멀티테넌트 데이터 시스템. Hadoop 모듈인 HDFS, YARN, MapReduce를 비롯해 ZooKeeper, Oozie, Spark, Hive 사용 지원
* **CQuery**: C3의 HDFS에 테이블 형식으로 데이터를 저장하고 SQL 분석 지원
* **Logiss**: 로그 수집과 실시간 검색을 제공하며 Cuve, CQuery에 데이터 연동 지원
* **AiSuite**: GPU 인프라와 Kubeflow를 지원하고 C3의 secure HDFS, Ceph, localPath, JuiceFS 등 용도에 맞는 스토리지 지원

Logiss는 OpenSearch와 OpenSearch Dashboards를 이용해 실시간 인덱싱과 검색 기능을 제공합니다. 또한 Logstash, Kafka, Storm, Java 애플리케이션으로 구성한 파이프라인을 통해 OpenSearch와 랜딩 존(landing zone)에 데이터를 실시간 전송합니다. 랜딩 존은 Cuve, CQuery 등에서 운영합니다.

> 랜딩 존: 실시간 로그(단순 원본 데이터)를 SQL로 활용하기 전, 단기 저장을 목적으로 운영하는 저장 공간

![](https://d2.naver.com/content/images/2025/12/3.png)

다양한 컴포넌트로 구성된 Logiss의 파이프라인 아키텍처

* **FEL(FrontEnd Logstash)**: 사용자의 로그를 다양한 프로토콜(TCP, Beats, HTTP 등)로 수신해 Front Kafka에 적재
* **Traffic-Controller**: Storm의 두 가지 토폴로지
  + **main 토폴로지**: Front Kafka에 적재된 데이터를 지정된 Post Kafka의 실시간 처리 토픽으로 전달. 데이터마다 설정한 최대 처리 속도를 초과하지 않도록 하며, 초과 전송 분은 Post Kafka의 후처리를 위한 별도 토픽에 저장.
  + **retry 토폴로지**: 후처리 데이터를 다시 처리해 Post Kafka의 실시간 처리 토픽으로 전송
* **BEL(BackEnd Logstash)**: Post Kafka에서 OpenSearch로 인덱싱할 토픽들의 데이터를 소비해 OpenSearch로 벌크 인덱싱 요청
* **Kafcuve**: Java 앱으로, Post Kafka에서 랜딩 존으로 전송할 토픽을 소비해 랜딩 존으로 데이터 전송
* **LAP(Logiss Async Processor)**: Jenkins 배치(batch) 잡을 실행하는 주체. 전송 속도를 초과해 Post Kafka에 후처리 데이터로 쌓인 토픽의 데이터를 소비하고, 다시 정해진 속도로 Traffic-Controller의 retry 토폴로지로 데이터 전송.

Traffic-Controller는 로그 데이터마다 설정된 최대 전송 속도를 초과하지 않도록 뒷단으로 데이터를 흘려보냅니다. 데이터마다 전송 속도를 설정하는 이유는 Logiss가 공용 플랫폼이므로 뒷단의 부하가 어느 한 데이터에 점유되는 것을 막고, 과도한 트래픽으로부터 뒷단을 보호하기 위해서입니다.

기존 로그 파이프라인의 문제점
----------------

### 1. 단일 토폴로지로 운영되던 Traffic-Controller

트래픽 제어를 담당하는 Traffic-Controller의 main 토폴로지는 단일 토폴로지로 구성되어 있었습니다. Storm에 swap 기능이 없어, 배포하려면 실행 중인 토폴로지를 중단하고 다시 제출해야만 했습니다(참고: [Updating a running topology](https://storm.apache.org/releases/2.8.2/Running-topologies-on-a-production-cluster.html)).

단일 토폴로지 구조에서는 무중단 배포가 불가능했고, 이로 인해 배포 시 약 3~8분 정도 파이프라인 지연이 발생했습니다.

다음은 서로 다른 2개의 클러스터에 단일 토폴로지로 구성된 Traffic-Controller를 배포할 때 데이터를 소비하는 Front Kafka의 랙(lag) 그래프입니다. 평상시에는 랙이 거의 발생하지 않지만, 토폴로지를 재기동하는 동안에는 데이터 처리가 멈추어 처리해야 할 데이터가 쌓이는 모습을 확인할 수 있습니다.

![](https://d2.naver.com/content/images/2025/12/4.png)

서로 다른 2개의 클러스터에 단일 토폴로지로 구성된 Traffic-Controller를 배포할 때 발생한 Front Kafka 랙

실시간 파이프라인에 지연이 발생하기 때문에 항상 배포 전에는 Logiss 사용자에게 별도로 공지해야 했고, 사용자 영향이 적고 트래픽이 적은 새벽 시간에만 배포할 수 있었습니다. 예정된 상황에서는 미리 공지하고 작업할 수 있지만, 긴급 배포가 필요한 경우에는 지연을 피할 수 없다는 한계도 있었습니다.

또한 점진적 배포가 불가능하여, 실제 서비스 환경에서만 드러나는 부작용은 배포가 완전히 끝난 후에야 확인할 수 있었습니다.

### 2. 낮과 새벽의 트래픽 차이

Logiss는 네이버 전사 로그를 수집하기 때문에 일반적으로 낮 시간에 트래픽이 많고 새벽 시간에 적은 유입 패턴을 보입니다.

다음은 운영 중인 클러스터 중 하나의 7일치 Front Kafka 트래픽 유입 패턴입니다. 최대 트래픽이 최소 트래픽의 약 5배인 것을 확인할 수 있습니다.

![](https://d2.naver.com/content/images/2025/12/5.png)

특정 클러스터의 7일치 Front Kafka 트래픽 유입 패턴(초당 로그 개수, 초당 로그 크기를 임의 단위로 표시)

Logiss의 파이프라인은 물리 장비(PM)로 구성되어 있고, 실시간 트래픽 처리가 목표입니다. 따라서 낮 시간에 집중된 피크(peak) 트래픽을 여유 있게 처리할 수 있도록 클러스터 규모를 산정해야 하며, 이 기준에 따라 머신 리소스를 다소 높게 투입하여 운영해야 했습니다.

![](https://d2.naver.com/content/images/2025/12/6.png)

특정 클러스터의 7일치 Post Kafka 디스크 사용량, OpenSearch CPU 사용량(임의 단위로 표시)

이처럼 낮 시간에 트래픽이 집중되는 패턴 때문에 트래픽이 적은 새벽 시간대에는 머신 자원을 충분히 활용하지 못하는 비효율이 발생했습니다.

### 3. 모든 로그의 공평한 처리

Front Kafka는 단일 토픽으로 운영되고 있어 급작스러운 트래픽 유입이나 장비 이슈가 발생하면 트래픽 처리 지연이 발생할 수 있습니다. 이때 로그의 중요도와 상관없이 데이터마다 설정된 최대 처리 속도로 처리되었습니다.

급작스러운 트래픽 유입으로 지연이 발생하면, 지연된 데이터에는 실시간으로 빠르게 처리되어야 하는 사업·서비스 핵심 로그와 상대적으로 중요도가 낮아 천천히 처리해도 되는 로그가 함께 섞여 있습니다.

![](https://d2.naver.com/content/images/2025/12/7.png)

트래픽 지연이 발생했을 때의 Front Kafka 랙

### 4. 장기 저장소와 실시간 검색을 위한 저장소에 모두 전달

Logiss는 데이터의 목적이나 성격에 따라 OpenSearch에만 저장하거나, 랜딩 존에만 저장하거나, 양쪽 모두에 저장합니다. 특히 양쪽 모두 저장하도록 설정된 데이터 중에는 전체 트래픽을 저장하는 대신 일부 샘플링만으로도 충분히 유의미한 데이터를 얻을 수 있는 경우도 있었습니다.

하지만 전달받은 트래픽을 100% 저장하는 것이 기본 원칙이었기 때문에, 모든 트래픽을 저장할 필요가 없는 경우에도 저장소를 비효율적으로 사용했습니다.

![](https://d2.naver.com/content/images/2025/12/8.png)

데이터 A가 장기 저장소인 랜딩 존, 실시간 검색을 위한 저장소 OpenSearch 양쪽에 저장될 때 수신한 데이터를 100% 저장

### 문제점 정리

1. 단일 토폴로지로 운영되던 Traffic-Controller의 main 토폴로지 → 무중단, 점진적 배포 불가
2. 낮과 새벽의 트래픽 차이 → 낮 시간 트래픽을 기준으로 산정된 다소 과한 장비 리소스
3. 모든 로그의 공평한 처리 → 처리 지연 등 비상시, 중요한 로그와 덜 중요한 로그 모두 같이 지연
4. 장기 저장소와 실시간 검색을 위한 저장소에 모두 전달 → 비효율적인 저장소 사용

해결 방법
-----

### Storm Kafka spout의 변경 및 멀티 토폴로지 도입

storm-kafka-client(spout)는 1.x에서 2.x로 버전이 변경될 때, KafkaConsumer.subscribe API 호출을 제거하고 assign 방식을 채택했습니다.

* **subscribe**: 소비할 파티션의 결정(할당)을 Kafka 브로커에게 맡기는 방식
* **assign**: 소비할 파티션의 결정을 KafkaConsumer(spout)가 직접 결정하는 방식

2.x에서 subscribe API 호출을 제거하고 assign 방식을 채택한 이유(subscribe API의 단점)는 다음과 같습니다.

![](https://d2.naver.com/content/images/2025/12/9.png)

storm-kafka-client에서 주장한 subscribe API의 단점(관련 이슈: [[STORM-2542] Deprecate storm-kafka-client KafkaConsumer.subscribe API subscriptions on 1.x and remove them as options in 2.x](https://github.com/apache/storm/issues/6324))

* 당시 cooperative sticky, sticky 등 고도화된 파티션 할당 전략이 없어, 재할당 과정 중 전체 spout의 소비가 멈춤
* spout이 어떤 파티션을 처리할지 예측할 수 없음
* spout이 하나의 executor에서 여러 task와 함께 실행되거나, 하나의 스레드에 여러 KafkaConsumer가 있는 경우 시스템이 멈추거나 이상 동작
* executor crash가 발생하면 전체 파티션이 재할당되어 필요 이상의 중복이 발생하고, 처리하던 파티션에 커밋할 수 없음

하지만 assign API 채택은 다음과 같은 이유로 Traffic-Controller에 큰 이점이 없었습니다.

* Kafka에 고도화된 파티션 할당 전략 도입(파티션 리밸런스 시간 단축 및 최소화로 중복 최소화 가능)
* Traffic-Controller의 spout은 어떤 파티션을 처리할지 예측할 필요가 없음
* Traffic-Controller의 spout은 Kafka로부터 레코드를 소비하고 바로 다음 bolt로 넘겨주는 작업만 담당했으며, spout 하나에 하나의 KafkaConsumer로 동작

따라서 assign 방식이 적용된 storm-kafka-client 2.x 버전을 사용하는 Traffic-Controller에는 다음과 같은 문제점만 있었습니다.

* 토폴로지 ID만 바꾸어서 여러 개 띄울 경우, 토폴로지 개수만큼 파티션 중복 소비와 처리 발생
* 브로커가 파티션 할당을 담당하지 않기 때문에 파티션 할당 전략을 사용할 수 없음

이러한 assign 방식의 문제점 때문에 다시 subscribe 방식으로 돌아가기 위한 방법을 고민했습니다.

#### KafkaConsumer.subscribe 방식 적용을 위한 storm-kafka-client 다운그레이드 시도

멀티 토폴로지 도입을 위해 KafkaConsumer.assign을 subscribe로 변경하고자 시도한 첫 번째 방법은 storm-kafka-client를 2.x에서 1.1.0으로 다운그레이드하는 것이었습니다.

멀티 토폴로지를 구성해도 파티션 중복 소비나 처리는 발생하지 않았지만, 장애 테스트 시 중단 시간이 매우 긴 것(약 6분)을 확인했습니다. 따라서 다운그레이드는 합리적인 선택이 아니라고 판단했습니다.

![](https://d2.naver.com/content/images/2025/12/10.png)

storm-kafka-client 1.1.0(subscribe API)을 사용하면서 멀티 토폴로지를 구성한 후, supervisor 다운 시 발생한 Front Kafka 랙(위)과 Traffic-Controller의 소비(아래)

#### 최신의 storm-kafka-client에서 KafkaConsumer.assign → subscribe 변경

그래서 최신 storm-kafka-client에서 assign API를 subscribe로 변경했습니다. Storm 2.3.0에서 변경한 사항은 다음과 같습니다.

```
diff --git external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java  
index 391aeccb6..d8b8ab0c9 100644  
--- external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java
+++ external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java
@@ -325,6 +325,10 @@ public class KafkaSpout<K, V> extends BaseRichSpout {
         final int maxUncommittedOffsets = kafkaSpoutConfig.getMaxUncommittedOffsets();
         for (TopicPartition tp : assignment) {
             OffsetManager offsetManager = offsetManagers.get(tp);
+            if (offsetManager == null) {
+                //This partition is not assigned to this spout
+                continue;
+            }
             int numUncommittedOffsets = offsetManager.getNumUncommittedOffsets();
             if (numUncommittedOffsets < maxUncommittedOffsets) {
                 //Allow poll if the partition is not at the maxUncommittedOffsets limit
@@ -460,6 +464,9 @@ public class KafkaSpout<K, V> extends BaseRichSpout {
                             LOG.trace("Emitted tuple [{}] for record [{}]", tuple, record);
                         }
                     } else {
+                        if (!offsetManagers.containsKey(tp)) {
+                            return false;
+                        }
                         emitted.add(msgId);
                         offsetManagers.get(tp).addToEmitMsgs(msgId.offset());
                         if (isScheduled) {  // Was scheduled for retry and re-emitted, so remove from schedule.
@@ -620,19 +627,14 @@ public class KafkaSpout<K, V> extends BaseRichSpout {
     @Override
     public void activate() {
         try {
-            refreshAssignment();
+            consumer.subscribe(Collections.singletonList(getTopicsString()), rebalanceListener);
         } catch (InterruptException e) {
             throwKafkaConsumerInterruptedException();
         }
     }

     private void refreshAssignment() {
-        Set<TopicPartition> allPartitions = kafkaSpoutConfig.getTopicFilter().getAllSubscribedPartitions(consumer);
-        List<TopicPartition> allPartitionsSorted = new ArrayList<>(allPartitions);
-        Collections.sort(allPartitionsSorted, TopicPartitionComparator.INSTANCE);
-        Set<TopicPartition> assignedPartitions = kafkaSpoutConfig.getTopicPartitioner()
-            .getPartitionsForThisTask(allPartitionsSorted, context);
-        topicAssigner.assignPartitions(consumer, assignedPartitions, rebalanceListener);
+        topicAssigner.assignPartitions(consumer);
     }

     @Override
diff --git external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/TopicAssigner.java external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/TopicAssigner.java  
index 300adecec..2942c8a20 100644  
--- external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/TopicAssigner.java
+++ external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/TopicAssigner.java
@@ -34,17 +34,10 @@ public class TopicAssigner implements Serializable {
      * @param <K> The consumer key type
      * @param <V> The consumer value type
      * @param consumer The Kafka consumer to assign partitions to
-     * @param newAssignment The partitions to assign.
-     * @param listener The rebalance listener to call back on when the assignment changes
      */
-    public <K, V> void assignPartitions(Consumer<K, V> consumer, Set<TopicPartition> newAssignment,
-        ConsumerRebalanceListener listener) {
-        Set<TopicPartition> currentAssignment = consumer.assignment();
-        if (!newAssignment.equals(currentAssignment)) {
-            listener.onPartitionsRevoked(currentAssignment);
-            consumer.assign(newAssignment);
-            listener.onPartitionsAssigned(newAssignment);
-        }
+    public <K, V> void assignPartitions(Consumer<K, V> consumer) {
+        if (consumer.assignment().isEmpty())
+            consumer.poll(0);
     }

 }
```

또한 멀티 토폴로지를 적용했을 때 spout, bolt 등의 task 할당이 장비마다 고르게 일어날 수 있도록 커스텀 스케줄러를 작성했습니다.

Storm에서 기본으로 제공하는 스케줄러들만으로는 supervisor별로 원하는 상태의 task 구성을 만들기 어려웠습니다. 다음은 기본 스케줄러를 이용해 spout 1개, parser bolt 3개, throttle bolt 2개, kafka bolt 3개로 구성된 main 토폴로지를 6개 배포했을 때 task가 supervisor별 slot에 어떻게 할당되는지 가시화한 이미지입니다.

![](https://d2.naver.com/content/images/2025/12/11.gif)

spout 1개, parser bolt 3개, throttle bolt 2개, kafka bolt 3개로 구성된 main 토폴로지를 기본 스케줄러로 6개 배포했을 때 slot별 task 할당 과정

기본 스케줄러를 사용하면 최악의 경우 특정 spout, bolt task만으로 구성된 supervisor가 존재할 수 있습니다. 다음은 기본 스케줄러를 이용해 spout 1개, parser bolt 2개, throttle bolt 1개, kafka bolt 1개로 구성된 main 토폴로지를 10개 배포했을 때 task가 supervisor별 slot에 할당된 예입니다.

![](https://d2.naver.com/content/images/2025/12/12.png)

spout 1개, parser bolt 2개, throttle bolt 1개, kafka bolt 1개로 구성된 main 토폴로지를 기본 스케줄러로 10개 배포했을 때 slot별 task 할당 할당 결과

supervisor2, 3의 모든 slot에는 parser bolt만 할당되었습니다. 이렇게 되면 parser bolt는 상대적으로 리소스를 많이 사용하기 때문에 supervisor2, 3에 부하가 집중되고, 전체 처리 병목이 두 장비에서 발생할 수 있습니다. 리소스를 상대적으로 덜 소비하는 spout과 bolt로 구성된 장비는 여유로운데도 말이죠.

따라서 spout과 bolt를 기계적으로 할당하는 기본 방식을 사용할 수 없었고, supervisor별로 이미 할당된 task를 고려하여 지능적으로 할당해야 했습니다.

#### 커스텀 스케줄러 개발

커스텀 스케줄러는 장비별로 특정 bolt, spout이 가장 적게 할당된 supervisor를 찾아 할당하는 방식으로 구현했습니다.

다음은 스케줄러의 핵심 로직입니다. 컴포넌트(Front Kafka spout, throttle bolt 등)가 가장 적게 할당된 supervisor slot을 찾고, 동일한 경우 slot이 가장 여유로운(task가 가장 적은) 노드의 slot을 선택합니다.

```
private String findBestSupervisor(Map<String, Map<String, Integer>> supervisorComponentCounts,  
    Map<String, List<WorkerSlot>> availableSlotsBySupervisor, String component) {
    return availableSlotsBySupervisor.entrySet().stream()
        .filter(e -> !e.getValue().isEmpty()) // slot이 있는 supervisor만 대상
        .min(Comparator.comparingInt((Map.Entry<String, List<WorkerSlot>> e) ->
                supervisorComponentCounts.getOrDefault(e.getKey(), Collections.emptyMap())
                    .getOrDefault(component, 0)) // 1순위: 해당 컴포넌트 개수
            .thenComparingInt(e ->
                supervisorComponentCounts.getOrDefault(e.getKey(), Collections.emptyMap())
                    .values().stream().mapToInt(Integer::intValue).sum() // 2순위: 전체 task 수
            )
        )
        .map(Map.Entry::getKey)
        .orElse(null);
}
```

다음은 커스텀 스케줄러를 이용해 spout 1개, parser bolt 2개, throttle bolt 1개, kafka bolt 1개로 구성된 main 토폴로지를 10개 배포했을 때 task가 supervisor별 slot에 어떻게 할당되는지 가시화한 이미지입니다.

![](https://d2.naver.com/content/images/2025/12/13.gif)

spout 1개, parser bolt 2개, throttle bolt 1개, kafka bolt 1개로 구성된 main 토폴로지를 커스텀 스케줄러로 10개 배포했을 때 slot별 task 할당 과정

10개의 토폴로지가 모두 할당된 결과를 보면 각 장비별 bolt, spout의 개수가 최대한 같게 할당된 것을 확인할 수 있습니다. 각 supervisor에 2개의 throttle bolt가 할당되었습니다.

![](https://d2.naver.com/content/images/2025/12/14.png)

spout 1개, parser bolt 2개, throttle bolt 1개, kafka bolt 1개로 구성된 main 토폴로지를 커스텀 스케줄러로 10개 배포했을 때 slot별 task 할당 결과

#### subscribe API로 변경 + 커스텀 스케줄러가 적용된 멀티 토폴로지에서 다양한 장애 테스트 진행

멀티 토폴로지에 커스텀 스케줄러를 적용한 후, 기존 단일 토폴로지 대비 다양한 상황에서 지연이나 중복의 정도를 비교해 보기 위해 장애 테스트를 진행했습니다.

* supervisor 장비 다운을 모사하는 supervisor 중단
* 배포 상황에서 발생하는 topology kill
* executor crash를 모사하는 bolt, spout executor kill

단일 토폴로지(assign)와 멀티 토폴로지(subscribe with range assignor)의 장애 테스트 결과를 항목별로 비교하면 다음과 같습니다.

| 항목 | 단일 토폴로지 + assign | 멀티 토폴로지 + subscribe(range) | 멀티 토폴로지 + subscribe(range)의 특징 |
| --- | --- | --- | --- |
| supervisor 중단 | 전체 파티션 지연(75~115초) 약 45초 데이터에 대해 11% 중복 처리 | 전체 파티션 지연(30~90초) 약 45초 데이터에 대해 36% 중복 처리 | **지연이 다소 줄었으나,** **중복 처리 양이 더 많아짐** |
| topology kill | 전체 파티션 처리 중단 | 일부 파티션 지연(30초) | 전체 파티션의 처리가 멈추지는 않게 됨 |
| bolt executor kill | 전체 파티션 지연(60초) 약 5초 데이터에 대해 0.4% 중복 처리 | 일부 파티션 지연(30~60초) 약 10초 데이터에 대해 0.4% 중복 처리 | 단일 토폴로지 대비 큰 차이 없음 |
| spout executor kill | 전체 파티션 지연(60초) 약 5초 데이터에 대해 3% 중복 처리 | 전체 파티션 지연(30~90초) 약 5초 데이터에 대해 3% 중복 처리 | 단일 토폴로지 대비 큰 차이 없음 |

topology, spout executor, bolt executor kill의 세 가지 테스트에서는 일부 파티션의 처리만 지연되고, 중복 처리 양은 단일 토폴로지와 비슷하다는 큰 이점이 있었습니다. 하지만 평상시에도 흔하게 일어날 수 있는 장비 다운 등의 상황과 비슷한 supervisor 중단 테스트에서는 중복 처리가 단일 토폴로지보다 많은 것을 확인했습니다.

이렇게 다량의 데이터가 중복 처리되더라도, 실제 주요 로그의 경우에는 로그마다 고유 키를 부여하고, 이를 활용해서 랜딩 존 내부에서 중복 제거(deduplication) 작업이 수행되어 큰 문제가 되지는 않습니다. 그래도 OpenSearch에서의 중복이나 랜딩 존 내부의 중복 제거 작업을 최소화하기 위해 Traffic-Controller에서의 중복 처리는 최소로 줄여야 합니다.

![](https://d2.naver.com/content/images/2025/12/15.png)

랜딩 존 내부에서 중복 로그 제거

#### 중복을 줄이기 위한 파티션 할당 전략 수정

subscribe + 멀티 토폴로지(range assignor)에서 supervisor 중단 시 과한 중복이 발생하는 주 원인은 전체 파티션의 소유권 변경이었습니다. 중단한 supervisor가 처리하던 파티션(A)의 소유권을 다른 supervisor의 spout이 이어받아 처리하는데, 기본 파티션 할당 전략인 range assignor에서는 A 파티션뿐만 아니라 다른 모든 파티션의 소유권이 변경되므로 과도한 중복이 발생했습니다.

그래서 중단한 supervisor가 처리하던 파티션의 소유권만 변경되도록, sticky assignor를 파티션 할당 전략으로 적용하고 다시 장애 테스트를 진행했습니다.

다음은 단일 토폴로지와 멀티 토폴로지(with sticky assignor)에서 장애 테스트 시 Front Kafka의 파티션별 랙 오프셋 그래프입니다. 빨간색 동그라미는 supervisor 다운 시, 초록색 동그라미는 토폴로지를 하나 내렸다가 올렸을 때, 파란색 사각형은 spout과 bolt 3개를 차례로 내렸을 때입니다.

![](https://d2.naver.com/content/images/2025/12/16.png)

단일 토폴로지(위)와 멀티 토폴로지(with sticky assignor)(아래)에서 장애 테스트 시 Front Kafka의 파티션별 랙 오프셋

* 빨간색 원: supervisor 다운 시. 단일 토폴로지(위)의 처리 지연 시간이 깁니다.
* 초록색 원: 토폴로지 중단 시. 단일 토폴로지(위)에서는 모든 파티션의 처리가 중단되는 반면, 멀티 토폴로지(아래)에서는 일부 파티션의 처리만 중단됩니다.
* 파란색 사각형: spout과 bolt 3개를 차례로 내렸을 때. 단일 토폴로지(위)에서는 모든 spout, bolt가 종료될 때 모든 파티션의 처리가 중단되지만, 멀티 토폴로지(아래) bolt의 경우 일부 파티션만 처리 중단됩니다.

단일 토폴로지(assign)와 멀티 토폴로지(subscribe with sticky assignor)의 장애 테스트 결과를 항목별로 비교하면 다음과 같습니다.

| 항목 | 단일 토폴로지 + assign | 멀티 토폴로지 + subscribe(sticky) | 멀티 토폴로지 + subscribe(sticky)의 특징 |
| --- | --- | --- | --- |
| supervisor 중단 | 전체 파티션 지연(75~115초) 약 45초 데이터에 대해 11% 중복 처리 | 전체 파티션 지연(30~90초) 약 40초 데이터에 대해 13% 중복 처리 | **지연이 다소 줄고, 전체적인 중복 처리의 양이 비슷함** |
| topology kill | 전체 파티션 처리 중단 | 일부 파티션 지연(15~30초) | 전체 파티션의 처리가 멈추지는 않게 됨 |
| spout executor kill | 전체 파티션 지연(60초) 약 5초 데이터에 대해 3% 중복 처리 | 전체 파티션 지연(45~60초) 약 5초 데이터에 대해 0.68% 중복 처리 | 단일 토폴로지 대비 중복 처리의 양이 감소함 |
| bolt executor kill | 전체 파티션 지연(60초) 약 5초 데이터에 대해 0.4% 중복 처리 | 일부 파티션 지연(30~60초) 약 10초 데이터에 대해 0.4% 중복 처리 | 단일 토폴로지 대비 큰 차이 없음 |

이렇게 KafkaConsumer.assign을 subscribe로 변경하고, 커스텀 스케줄러를 구현하고, 비교적 최신 파티션 할당 전략인 sticky assignor를 적용하여 멀티 토폴로지를 도입했습니다. 다양한 장애 테스트 시 단일 토폴로지보다 파티션별 지연과 중복 처리 면에서 이점이 있었고, 특히 supervisor 다운 시 단일 토폴로지 대비 파티션 처리 지연이 감소하면서 중복 수준도 비슷한 수준으로 유지되었습니다.

### 데이터 처리 옵션과 클러스터 상태 개념 도입

앞서 설명한 기존 로그 파이프라인의 문제점 중 단일 토폴로지 문제를 제외하면 다음 세 가지 문제점이 있었습니다.

* **낮과 새벽의 트래픽 차이**: 낮 시간의 트래픽 중 일부를 새벽에 처리할 수 있다면?
* **모든 로그의 공평한 처리**: 데이터의 중요도에 따라 차등 처리를 할 수 있다면?
* **장기 저장소와 실시간 검색을 위한 저장소에 모두 전달**: 각 저장소별로 정해진 비율만큼만 저장할 수 있는 기능이 있다면?

각 문제점을 해결하기 위해 2가지 클러스터 상태 개념과 4가지 데이터 처리 옵션을 도입했습니다.

| 데이터 처리 옵션 | 클러스터 상태 |
| --- | --- |
| 처리 중단 허용 | backpressure |
| 우선순위 | mayday |
| 실시간 보장 비율 |  |
| OpenSearch 샘플링 비율 랜딩 존 샘플링 비율 |  |

#### 처리 중단 허용, 실시간 보장 비율, backpressure를 활용한 비실시간 처리

부하(backpressure) 기반 비실시간 처리의 목적은 트래픽 변화에 따라 낮 시간에 집중된 리소스 사용률을 한가한 시간대로 분산함으로써 피크 리소스 사용률을 낮춰 비용을 절감하는 것입니다.

동작 방식은 다음과 같습니다.

1. backpressure는 랜딩 존으로 흐르는 파이프라인과 OpenSearch로 흐르는 파이프라인의 부하 상태를 감지하여 활성화됩니다.
2. backpressure가 활성화되면, 처리 중단을 허용한 데이터는 설정된 최대 처리 속도 대비 실시간 보장 비율만큼 실시간 처리하고, 나머지는 후처리를 위한 Post Kafka의 별도 토픽에 쌓습니다(Post Kafka 이후 파이프라인에서 낮 시간 부하가 감소할 것을 기대합니다).
3. backpressure가 비활성화되면(주로 새벽 시간), 쌓인 토픽 데이터에 대해 설정된 최대 처리 속도의 두 배까지 후처리합니다(새벽 시간의 리소스 사용률이 높아지기를 기대합니다).

#### 우선순위, 실시간 보장 비율, mayday를 활용한 데이터의 차등 처리

비상(mayday 활성) 시 데이터 차등 처리의 목적은 로그별 우선순위를 설정하고 비상시 우선순위에 따라 차등 처리함으로써 서비스와 직결되어 중요도가 높은 로그의 지연을 최소화하고 상대적으로 중요도가 낮은 로그는 천천히 처리해 파이프라인 지연을 효율적으로 해소하는 것입니다.

Front Kafka와 Post Kafka의 컨슈머별 랙 오프셋을 주기적으로 모니터링하다가 일정 수치를 넘어가면 mayday가 활성화됩니다. 즉, 파이프라인 지연이 일정 수준을 넘으면 활성화됩니다.

![](https://d2.naver.com/content/images/2025/12/17.png)

Front Kafka, Post Kafka의 컨슈머 랙 오프셋을 모니터링해 일정 수치를 넘어가면 mayday 활성화

mayday가 활성화되면, 각 데이터는 우선순위와 실시간 보장 비율에 따라 다음과 같이 처리됩니다.

| 우선순위 | mayday 활성화 시 최대 처리 속도 |
| --- | --- |
| 1 | 설정된 최대 처리 속도의 5배까지 처리 |
| 2 | 설정된 최대 처리 속도만큼 처리 |
| 3 | 설정된 최대 처리 속도 X 실시간 보장 비율 만큼만 실시간 처리 예: 초당 1000개의 처리 속도, 50% 실시간 보장 → 초당 500개 처리 보장 |

이렇게 비상시 데이터의 우선순위에 따라 차등 처리를 수행하고, 우선순위가 높은 데이터의 지연을 최소화하고자 했습니다.

#### 저장소별 샘플링 비율을 이용한 샘플링 기능

OpenSearch와 랜딩 존의 샘플링 비율(sample rate)을 각 데이터별로 설정할 수 있도록 하여, 각 저장소별 저장 비율을 제어할 수 있게 했습니다. 그 결과, 100% 저장이 필요 없는 데이터에 대해서는 전달받은 데이터 중 설정된 비율만큼만 저장해 저장소를 효율적으로 활용할 수 있습니다.

성과
--

### 무중단, 점진적 배포 실현

단일 토폴로지에서 멀티 토폴로지로 전환하면서 일부 파티션의 일시적 중단만 발생하게 되었고, 무중단에 가까운 점진적 배포가 가능해졌습니다. 그 결과, 일부 토폴로지만 변경된 상태로 유지할 수 있게 되어, 전체 토폴로지 업데이트 전 사이드 이펙트 파악과 롤백이 한층 쉬워졌습니다.

![](https://d2.naver.com/content/images/2025/12/18.gif)

단일 토폴로지와 멀티 토폴로지 비교, 멀티 토폴로지의 롤링 리스타트

실제 토폴로지 롤링 리스타트 배포 시 Front Kafka의 파티션별 랙 오프셋 그래프는 다음과 같습니다.

![](https://d2.naver.com/content/images/2025/12/19.png)

멀티 토폴로지의 Traffic-Controller 롤링 리스타트 배포 시 Front Kafka의 파티션별 랙 오프셋

전체 파티션 중 일부 파티션의 랙 오프셋이 증가하는 부분은 각 토폴로지를 중단하고 처리를 재개하는 동안 일부 파티션의 일시적 지연이 발생하는 것을 의미합니다. 대부분 파티션의 랙 오프셋이 일정하게 유지되는 부분을 보면 다른 토폴로지의 처리는 일시적 중단도 없이 처리되는 것을 알 수 있습니다.

같은 시간에 Front Kafka 트래픽 in, out 속도는 다음과 같습니다(Front Kafka 트래픽의 out 속도 = Traffic-Controller의 소비 속도).

![](https://d2.naver.com/content/images/2025/12/20.png)

멀티 토폴로지의 Traffic-Controller 롤링 리스타트 배포 시 Front Kafka의 트래픽 in(왼쪽), out(오른쪽) 속도 비교

Front Kafka의 트래픽 in, out 속도가 대체로 같다는 것을 확인할 수 있습니다. 이는 토폴로지가 하나씩 배포(중단 → 처리 재개 반복)되는 과정에서 Traffic-Controller의 처리가 거의 지연 없이 이루어진다는 것을 의미합니다.

### 실시간/비실시간 처리의 분리

처리 중단을 허용한 로그를 비실시간으로 처리할 수 있게 되어, 낮 시간 피크 트래픽을 한가한 시간으로 옮겨 처리할 수 있게 되었습니다.

backpressure 상태에 따라 처리 중단을 허용한 로그의 처리 양상은 다음과 같습니다.

![](https://d2.naver.com/content/images/2025/12/21.png)

처리 중단을 허용한 로그의 비실시간 처리

backpressure가 활성화되었을 때 약 35Kcps를 초과하는 트래픽은 후처리를 위해 쌓아 두고, backpressure가 해제되었을 때 쌓아 둔 트래픽을 처리하는 것을 확인할 수 있습니다.

비실시간 처리 전후로, 쌓아둔 데이터를 처리하는 Traffic-Controller의 retry 토폴로지의 bolt 처리량·사용률 변화를 살펴보면 다음과 같습니다.

![](https://d2.naver.com/content/images/2025/12/22.gif)

비실시간 처리 전후의 Traffic-Controller의 retry 토폴로지의 bolt 처리량·사용률

처리 중단을 허용한 로그는 낮 시간에 backpressure가 활성화되면 실시간 보장 비율만큼만 실시간 처리하고 나머지는 후처리를 위해 쌓아 둡니다. 그리고 새벽 시간에 backpressure가 비활성화되면 최대 처리 속도의 두 배까지 후처리합니다. 이로 인해, 후처리를 담당하는 Traffic-Controller의 retry 토폴로지의 bolt 처리량과 사용률이 새벽 시간에 크게 증가한 것을 확인할 수 있습니다.

### 우선순위 기반 처리로 핵심 서비스 영향 최소화

파이프라인을 개선한 이후 실제 장애 상황은 아직 경험하지 않았기 때문에, 장애 상황을 시뮬레이션한 결과를 공유합니다. 시뮬레이션 과정은 다음과 같습니다.

먼저, 다음 표와 같이 우선순위 1, 2, 3으로 지정된 네 가지 데이터를 준비했습니다. 우선순위가 3인 두 가지 데이터의 실시간 보장 비율은 각각 60%와 30%로 설정했습니다. 네 가지 데이터 모두 OpenSearch와 랜딩 존에 저장되도록 설정하고, 최대 처리 속도는 초당 1200개로 설정한 뒤 실제 로그 전송은 초당 1000개씩 발생시켰습니다.

| 우선순위 | 최대 처리 속도(초당 개수) | 실제 전송 속도(초당 개수) | (mayday 발생 시) 실시간 보장 비율 |
| --- | --- | --- | --- |
| 1 | 1200 | 1000 | 500% |
| 2 | 1200 | 1000 | 100% |
| 3 | 1200 | 1000 | 60% |
| 3 | 1200 | 1000 | 30% |

위와 같이 15분 동안 로그를 전송한 후, 전송을 계속 유지한 상태에서 Traffic-Controller 중단 → Front Kafka의 컨슈머 오프셋을 10분 전으로 설정 → Traffic-Controller 재시작 작업을 수행해 mayday 상황을 모사했습니다.

![](https://d2.naver.com/content/images/2025/12/23.png)

mayday 상황 시뮬레이션에서 Front Kafka 랙

23시 55분경 Traffic-Controller를 중단한 뒤, 23시 56분경 컨슈머 오프셋을 10분 전으로 설정하는 순간 랙이 증가했고, 이후 23시 57분경 Traffic-Controller의 처리가 재개되면서 랙이 감소하는 것을 확인했습니다.

Traffic-Controller 중단하고 Front Kafka의 컨슈머 오프셋을 10분 전으로 설정하는 과정에서 곧바로 클러스터 상태인 mayday가 활성화되었고, Traffic-Controller 재시작과 동시에 각 데이터는 Traffic-Controller가 처리할 수 있는 최대 처리 속도(4.2Kcps)로 처리되기 시작했습니다.

이때, 실제 뒷단인 OpenSearch와 랜딩 존으로의 전송 속도는 우선순위에 따라 차등이 있었습니다. 우선순위가 낮은 데이터는 실시간 보장 비율만큼만 전달하고, 우선순위가 높은 데이터는 지연을 줄이고 가능한 한 실시간으로 처리하는 것을 확인했습니다.

![](https://d2.naver.com/content/images/2025/12/24.png)

mayday 상황에서 우선순위별 Traffic-Controller의 처리 속도(위), 우선순위별 실제 뒷단으로의 전송 속도(아래)

### 샘플링 기능으로 저장소 효율화

저장소별로 샘플링 비율을 설정하는 기능을 제공함으로써 저장소를 효율적으로 이용할 수 있게 되었습니다.

![](https://d2.naver.com/content/images/2025/12/25.png)

OpenSearch 샘플링 비율 조정(8월 14일 오후) 전후의 OpenSearch 사용량(문서 수, 크기)

샘플링 비율을 조정한 후 실제로 저장소 사용량이 감소하는 것을 확인했습니다.(일마다 자정 이후에 문서 수와 크기가 급감하는 것은 Index State Management 정책에 의해 오래된 인덱스가 삭제되는 것과 관련이 있습니다.)

마치며
---

storm-kafka-client 라이브러리를 수정하고 커스텀 스케줄러를 작성해, 단일 토폴로지로 운영하던 Storm의 토폴로지를 멀티 토폴로지로 변경했습니다. 그 결과 무중단, 점진적 배포가 가능해졌습니다.

또한 다양한 데이터 처리 옵션과 클러스터 상태 개념을 도입해 다음과 같은 문제점을 해결했습니다.

* 낮 시간에 집중된 트래픽을 한가한 시간에 처리
* 트래픽 지연 등 비상 상황에서 데이터의 우선순위에 따라 처리 속도에 차등을 두어, 우선순위가 높은 데이터의 지연을 최소화
* 저장소별 샘플링 비율 설정 기능을 제공해 저장소를 효율적으로 사용

### 데이터 처리 옵션 변경의 필요성

장비는 한정되어 있고, 새로 도입한 지능형 파이프라인이 비상시와 평시에 모두 효율적으로 동작하려면 데이터마다 적절한 처리 옵션을 지정해야 한다는 한계가 있습니다.

지능형 파이프라인 도입으로 기대하는 수준은 다음과 같습니다.

* **비실시간 처리**: 낮 시간의 피크 트래픽이 새벽으로 많이 옮겨져, 낮과 새벽에 처리되는 트래픽 규모가 비슷한 수준이 되기를 기대합니다.

![](https://d2.naver.com/content/images/2025/12/26.gif)

낮 시간에 집중된 트래픽을 한가한 새벽 시간에 처리하는 이상적인 모습을 표현하는 애니메이션

* **비상시 우선순위 기반 처리**: 시뮬레이션 결과와 같이 우선순위가 높은 데이터의 지연이 최소화되기를 기대합니다.

![](https://d2.naver.com/content/images/2025/12/27.png)

처리 지연 등의 mayday 상황에서 데이터가 우선순위별 차등 속도로 전송되는 이상적인 모습

비실시간 처리로 낮·새벽 시간의 트래픽 처리량이 비슷한 수준이 되려면 처리 중단을 허용한 로그가 충분히 많아야 하고, 비상시 우선순위 기반 처리가 적절히 수행되려면 데이터의 우선순위가 고르게 분포해야 한다는 한계가 있습니다.

그래서 사용자 데이터 특성에 맞게 데이터 처리 옵션을 잘 설정할 수 있도록 사내에 여러 가지 안내와 홍보를 진행할 예정입니다. 무분별한 우선순위 지정을 막기 위해, 우선순위는 로그 데이터가 서비스에 이용되는지(주요 지표 또는 실시간 학습·집계에 필요한 데이터) 여부 등을 고려해 로그 전송처와 운영자가 협의하여 결정합니다.

### 향후 계획 - AI 기반 클러스터 상태 전환

현재는 지정된 임계값을 초과하거나 하회할 때 backpressure, mayday 등의 클러스터 상태가 변경됩니다. 이 방식은 임계값 근방에서 트래픽이나 지연이 유지되는 경우 상태가 너무 자주 변경될 수 있습니다.

또한 클러스터 규모가 변경되거나 클러스터마다 지연을 허용하는 수준이 달라지는 경우도 있습니다. 이 경우에는 운영자가 임계값을 직접 조정해야 합니다.

향후에는 사람의 개입 없이 트래픽과 지연 관련 시계열 데이터를 AI에게 학습시켜, 클러스터 상태를 AI가 스스로 판단하고 전환할 수 있도록 개선해 볼 계획입니다.
