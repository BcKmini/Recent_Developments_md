# AI로 E2E 테스트를 찍어내다: MAFT

**출처:** [Naver_D2](https://d2.naver.com/helloworld/3088532)

## 요약
“API 문서만 주면 테스트 코드가 생성되고 자동으로 PR까지 생성된다면 얼마나 좋을까?”

MAFT(Multi Agents For Testing)는 이런 바람에서 출발한 사이드 프로젝트로, LLM 기반 에이전트들이 E2E(End-to-End) 테스트 코드 생성을 여러 작은 서브 태스크로 분리하고 수행하는 파이프라인입니다.

![](https://d2.naver.com/content/images/2025/10/1.png)

MAFT는 개별 데이터 특화 검색 엔진 Noir을 위한 프로젝트로 시작했습니다. Noir는 네이버 메일, 메시지 검색 서비스 등에 사용하는 검색 엔진으로, 여러 기술적 문제를 해결하며 안정적으로 개발 및 운영되고 있습니다.

> Noir 및 Noir가 해결한 기술적 문제에 대해 더 알고 싶다면 다음 글을 참고해 주세요.
>
> * [Noir 제작기](https://deview.kr/2023/sessions/566)
> * [Go 프로파일링 적용기](https://d2.naver.com/helloworld/8404108)
> * [Go 메모리 누수 탐지 및 GC 주기 조절 사례](https://d2.naver.com/helloworld/5316262)

MAFT는 Noir의 다음 목표 중 하나인 '충분한 E2E 테스트 구축'을 위한 별도의 프로젝트입니다. MAFT는 Noir의 API 1개당 평균 10개 이상의 테스트 케이스를 생성했고 2개 이상의 버그를 찾는 성과를 보였습니다. 수십 개의 API에 대해서도 쉽게 테스트를 생성할 수 있기 때문에 앞으로 Noir 개발에 큰 도움이 될 것이라 기대합니다.

이 글에서는 MAFT를 개발하며 얻은 경험과 현재 MAFT의 한계점을 공유하고자 합니다. 누구나 LLM API를 활용해 쉽게 애플리케이션을 만들 수 있는 시대에, 이 기록이 같은 고민을 하는 분들께 작은 도움이 되기를 바랍니다.

MAFT의 목표
--------

MAFT를 구상하게 된 계기는 Noir의 운영 API 다수에서 발생한 테스트 공백이었습니다. 이는 검색 엔진이라는 특성상 서비스 품질에 직접적인 리스크가 될 수 있습니다.

Noir는 검색 엔진 특성상 API 간 의존 관계가 강합니다. 예를 들어 검색 API를 테스트하려면 먼저 문서 입력 API로 데이터를 입력해야 합니다. 새로운 API를 하나 추가할 때마다 의존하는 API들이 생기고, 그 API들이 의존하는 다른 API들로부터도 영향을 받아 확인할 케이스가 점점 많아집니다. 이런 상황에서 사람이 일일이 모든 시나리오를 설계하고 테스트 코드를 작성하는 것은 매우 비효율적입니다.

MAFT는 개발자를 대신하여 API 간 의존 관계를 파악하고 그에 대응하는 테스트 케이스를 만드는 것을 목표로 했습니다. 또한, 시작은 Noir의 문제를 풀기 위함이었으나 장기적으로는 API 문서가 존재하는 모든 애플리케이션을 위한 파이프라인이 되고자 했습니다.

따라서 MAFT의 목표를 다음과 같이 설정했습니다.

* API 문서로부터 E2E 테스트 코드 생성하는 과정 중 개발자 개입 없는 자동화 달성
* 생성된 테스트 코드의 높은 실행 가능성과 커버리지 확보
* 사람이 직접 작성하는 것 대비 시간·비용의 유의미한 절감

MAFT는 단순한 코드 생성에 그치지 않고 다음과 같은 확장성도 같이 지원하고자 했습니다.

* 사용자가 컨텍스트를 보강할 수 있는 설명 입력 지원
* 필요에 따라 에이전트를 자유롭게 추가/제거할 수 있는 모듈형 설계
* Github Actions와 연동을 통한 간편한 운영 환경

멀티 에이전트 프레임워크 선택
----------------

MAFT는 여러 작은 에이전트가 협업하여 E2E 테스트를 생성하는 프로젝트입니다. 에이전트를 효율적으로 관리하기 위해서 멀티 에이전트 프레임워크인 AutoGen을 활용해 구현했습니다.

'LLM'과 'multi agent'라는 키워드로 검색하면 수많은 프레임워크를 쉽게 찾을 수 있습니다. 초기 MAFT MVP를 구현할 때 추렸던 후보는 문서의 품질 및 스타 수가 월등한 [AutoGen](https://github.com/microsoft/autogen), [MetaGPT](https://github.com/geekan/MetaGPT), [LangGraph](https://github.com/langchain-ai/langgraph)였습니다. 이 세 개의 프레임워크는 지금도 빠르게 업데이트되고 있어, 해당 설명이 현재는 유효하지 않을 수 있습니다. AutoGen의 경우, 메인테이너인 Microsoft가 새로운 에이전트 프레임워크로 통합할 것이고 더 이상 새로운 기능이 추가되지 않는다고 밝히기도 했습니다([참고](https://github.com/microsoft/autogen/discussions/7066)).

| 프레임워크 | 메인테이너 | 특징 | 프롬프트 디버깅 툴 |
| --- | --- | --- | --- |
| AutoGen | Microsoft | 도구 기반 책임 에이전트 | Phoenix |
| MetaGPT | MGX | 역할 기반 책임 에이전트 | 존재하지 않음 |
| LangGraph | LangChain | 그래프 커스터마이즈 용이 | LangSmith |

세 프레임워크를 사용해 간단한 예를 작성해 보았을 때 LangGraph는 AutoGen과 MetaGPT에 비해 복잡하다고 느껴 AutoGen과 MetaGPT 중에서 선택했습니다.

제가 느낀 AutoGen과 MetaGPT의 가장 큰 차이점은 에이전트의 책임을 나눌 때 도구 기반으로 책임을 나누는가, 아니면 역할 기반으로 책임을 나누는가였습니다. 이 두 라이브러리를 처음 보신다면 도구 기반 책임과 역할 기반 책임이 무엇인지 잘 와닿지 않으실 것입니다.

AutoGen 프레임워크는 에이전트에게 도구를 부여하고 그 도구가 곧 에이전트의 역할이 됩니다. AutoGen이 제공하는 대표적인 Agent 클래스로는 UserProxyAgent(사용자의 입력을 전달하는 에이전트), WebSurfer(웹을 검색하는 에이전트), CodeExecution(코드 실행 에이전트)가 있습니다. 시스템 프롬프트를 커스텀하여 에이전트의 역할을 조정할 수 있지만 기본적으로 기대하는 역할은 에이전트가 제공받은 도구를 실행하는 것입니다.

반면 MetaGPT는 에이전트를 한층 더 추상화된 형태로 봅니다. MetaGPT가 제공하는 대표적인 Agent 클래스로는 ProductManager, DataInterpreter 등이 있습니다. MetaGPT는 Agent 클래스를 'Role'이라고 합니다. MetaGPT는 에이전트를 '소프트웨어 회사의 구성원'으로 보고, 에이전트의 도구보다는 어떠한 역할을 수행하는지를 중요하게 생각합니다. 그래서 역할이 다른 에이전트가 같은 도구를 사용하는 경우도 생길 수 있습니다.

역할 기반 책임보다 도구 기반 책임이 더 직관적으로 느껴져 AutoGen을 최종 선택했습니다.

MAFT 구성
-------

MAFT의 구성은 다음과 같습니다.(MODEL CONNECT는 사내 LLM API provider입니다.)

![](https://d2.naver.com/content/images/2025/10/2.png)

### Github Action 통합

사용자는 손쉽게 Github Action 워크플로로 MAFT를 실행할 수 있습니다.

Github Action 실행기는 애플리케이션 API 문서를 담은 디렉터리와 테스트 코드를 담을 디렉터리를 docker 컨테이너에 마운트한 후 MAFT 이미지를 실행합니다. 이후 MAFT가 완료되어 디렉터리에 테스트 코드가 담기면 저장소에 PR을 생성합니다.

이를 통해 테스트 생성을 자동화하고 개발자가 이를 검증한 뒤 머지하는 사이클을 손쉽게 만들 수 있습니다.

### 에이전트

![](https://d2.naver.com/content/images/2025/10/3.png)

MAFT는 여러 개의 에이전트가 대화를 주고받으며 작업을 수행합니다.

에이전트는 크게 시스템 프롬프트, 사용자 프롬프트, 도구로 이루어져 있습니다. 시스템 프롬프트와 사용자 프롬프트, 그리고 도구들에 대한 설명은 LLM의 컨텍스트에 포함됩니다.

시스템 프롬프트는 에이전트의 주 역할을 정의합니다. “당신은 E2E 테스트 코드 생성 에이전트입니다. 이전 스텝에서 생성한 테스트 시나리오를 바탕으로 E2E 테스트 코드를 작성해야 합니다.”와 같이 작성합니다.

사용자 프롬프트는 사용자가 자신의 의도에 맞게 정보를 추가하는 프롬프트입니다. 예를 들어 “테스트 대상 애플리케이션 Noir는 개인화 검색 엔진입니다. Noir는 작은 볼륨이 수십만 개 이상 있을 수 있다는 것을 염두에 두고 테스트 코드를 작성해야 합니다.”와 같은 애플리케이션 특화 정보 등을 추가할 수 있습니다.

도구는 에이전트가 LLM 외에 사용할 수 있는 기능입니다. LLM은 도구 설명을 보고 사용할 도구를 결정해 그 도구를 사용합니다. 도구 실행 결과는 다시 LLM의 컨텍스트에 포함되어 에이전트의 역할 수행에 도움을 줍니다. 간단하게 LLM이 어떻게 호출할 도구를 결정하는지 살펴보겠습니다.

1. 먼저 도구를 모두 스키마로 변경합니다.(<https://github.com/microsoft/autogen/blob/main/python/packages/autogen-ext/src/autogen_ext/models/openai/_openai_client.py#L244-L271>)
2. LLM API에 이 도구 스키마들을 담아 요청합니다.(<https://github.com/microsoft/autogen/blob/13e144e5476a76ca0d76bf4f07a6401d133a03ed/python/packages/autogen-ext/src/autogen_ext/models/openai/_openai_client.py#L693-L700>).
3. LLM 응답에는 어떤 도구를 호출할지 담겨 있으므로 이를 보고 적절한 도구를 선택해 호출합니다.(<https://github.com/microsoft/autogen/blob/13e144e5476a76ca0d76bf4f07a6401d133a03ed/python/packages/autogen-ext/src/autogen_ext/models/openai/_openai_client.py#L744-L774>)

적절한 도구가 선택되면 연결된 작업을 수행합니다. 예를 들어 파일을 다루는 에이전트면 파일 시스템 API를 호출하고, 웹 브라우징 에이전트면 chromium 브라우저를 실행하고, 코드 실행 에이전트면 서브 프로세스에서 셸을 열어 코드를 실행합니다.

MAFT 워크플로
---------

MAFT는 총 5개의 스텝을 거쳐 E2E 테스트 코드를 생성합니다. 스텝마다 역할을 부여받은 에이전트가 있으며, 이들이 서로 대화하며 작업을 수행합니다.

1. 요약 문서 생성
2. API 의존성 문서 생성
3. 테스트 시나리오 생성
4. E2E 테스트 코드 생성
5. 테스트 코드 검증

스텝을 나눈 이유는 LLM에게 한 번에 큰 단위의 일을 시킬 때보다 작은 단위로 쪼개서 일을 시킬 때 LLM이 훨씬 더 정확하게 일을 수행하기 때문입니다. 스텝을 나누는 것까지 LLM에게 시킬 수도 있지만 'E2E 테스트를 생성한다'라는 명확한 목표가 있어 스텝이 동적으로 변경될 필요가 없으므로 스텝을 고정했습니다. 만약 유닛 테스트 생성, 성능 테스트 생성 등 좀 더 넓은 범위로 확장한다면 LLM이 스텝을 나누게 하는 것도 고려할 수 있습니다.

### 스텝 구성 에이전트

각 스텝은 크게 세 종류의 에이전트로 구성되어 있습니다. 작업 시작 에이전트, 작업 비평 에이전트, 작업 수정 에이전트입니다.

![](https://d2.naver.com/content/images/2025/10/4.png)

* 작업 시작 에이전트는 그 단계의 주 작업을 수행합니다. 처음 작업을 수행하는 것이기에 이 단계에서 나온 작업물은 잘못됐거나 충분하지 않은 경우가 많습니다.
* 작업 비평 에이전트는 작업 시작 에이전트를 보완하는 역할을 합니다. 작업 비평 에이전트는 만들어진 결과물을 보고 잘못된 점은 무엇인지, 어떤 점이 부족한지 피드백을 남깁니다.
* 작업 수정 에이전트는 작업 비평 에이전트의 피드백을 보고 작업물을 수정합니다.

작업 시작 에이전트와 작업 수정 에이전트는 결과물을 만드는 에이전트이기 때문에 좋은 모델을 쓰는 것이 중요합니다. 반면 작업 비평 에이전트는 상대적으로 가벼운 모델을 사용하더라도 충분했습니다.

작업 시작 에이전트가 작업 수정 에이전트의 역할까지 같이 수행하도록 하지 않은 이유는 프롬프트가 미세하게 다르기 때문입니다. 작업 수정 에이전트는 피드백을 보고 작업물을 수정해야 하지만 작업 시작 에이전트는 피드백을 보지 않아도 됩니다. 두 역할을 모두 수행하도록 프롬프트를 작성하면 에이전트가 자신의 역할을 혼란스러워하는 경우가 많아 에이전트를 분리했습니다.

### 사용자 입력 문서

MAFT를 사용하기 위해 사용자가 준비할 것은 API 문서와 커스텀 정보입니다.

이때 테스트 대상 API뿐만 아니라 애플리케이션의 모든 API 문서가 필요합니다. 테스트 대상 API에 영향을 주는 다른 API도 포함해 테스트 케이스를 작성해야 하기 때문입니다.

```
## Common Information
// 모든 API에 공통적으로 적용되는 정보를 정리

### Request Format
// Content-Type, 문자 인코딩 방식 등 헤더 정보 포함
...

### Response Format
// Content-Type, 문자 인코딩 방식 등 헤더 정보 포함
...

### Common Response Codes
// 상태 코드와 설명을 표로 정리
...

## API Endpoints

### 1. Search Documents

#### Overview
// 메서드, 엔드포인트, 설명 등을 간단하게 정리
...

#### Request

##### Request Body
// 에이전트가 이해하기 쉽도록 요청 본문 예시를 포함
...

##### Request Parameters
// 쿼리 파라미터와 요청 본문 파라미터를 구분해 표로 정리
...

#### Response
// 성공 응답, 실패 응답을 구분해 정리
...
```

커스텀 정보는 에이전트가 테스트 작성 시 알고 있어야 할 추가 정보입니다. 테스트 코드 네이밍 컨벤션, 테스트 성공 조건(응답의 상태 코드만 검사할지, 응답 본문까지 검사할지 등), 테스트 코드 작성 시 사용할 라이브러리 지정 등 개발자가 추가로 원하는 모든 조건을 추가합니다. MAFT는 해당 정보를 모든 에이전트의 프롬프트에 주입합니다.

```
## Introduction
...

## Naming Convention of test code
...

## Caution when writing test code
...
```

### 중간 결과물

MAFT는 각 스텝을 진행하며 중간 결과물을 생성하고, 이 결과물을 사용해 최종 E2E 테스트를 만듭니다.

| 중간 결과물 | 생성 스텝 | 설명 |
| --- | --- | --- |
| 요약 문서 | 1번 스텝 | API 호출 시 필요한 정보를 요약한 문서 |
| API 의존성 문서 | 2번 스텝 | API 결과에 영향을 줄 수 있는 API를 정리한 문서 |
| 테스트 시나리오 | 3번 스텝 | 테스트 시나리오와 선행 호출 API, 검증 방법 등을 정리한 문서 |

#### 요약 문서

요약 문서는 API의 설명, 엔드포인트, 파라미터, 주의점 등을 담고 있습니다.

```
{
  "apiName": "Search_Documents",
  "endpoint": "...",
  "description": "...",
  "query parameters": [],
  "body parameters": [
    {
      "name": "...",
      "description": "..."
    },
    {
      "name": "...",
      "description": "..."
    },
    ...
  ],
  "success response": "...",
  "error response": "...",
  "limitations": "...",
  "note": "..."
}
```

사람이 작성한 애플리케이션 API 문서는 형식이 일관되지 않고 테스트 작성에 불필요한 정보가 담겨있는 경우가 많습니다. LLM의 컨텍스트 안에 일관되지 않고 불필요한 정보가 많을수록 잘못된 테스트를 만들 확률이 높아집니다.

그래서 API 문서에서 테스트 작성에 필요한 API 정보만을 추출해 표준화된 구조로 요약된 문서를 만들고, 이후 단계에서는 요약된 문서를 사용해 작업을 수행합니다. 다만, 요약된 문서에 누락된 정보가 있을 수 있으므로, 이후 단계에서 비평 에이전트는 원본 문서를 참고해 잘못된 부분이 없는지 검토합니다.

#### API 의존성 문서

API 의존성 문서는 요약 문서를 바탕으로 테스트 대상 API와 관련 있는 API를 정리한 문서입니다. 관련 있는 API(requiredBefore)가 테스트 대상 API 호출 전 몇 번째 순서로 호출되었을 때 테스트 대상 API에 어떤 영향을 미칠 수 있는지 정리합니다.

```
{
  "apiName": "Search_Documents",
  ...
  "description": "...",
  "relation_cases": [
    {
      "case": "case1",
      "parameters": "...",
      "requiredBefore": [
        {
          "apiName": "...",
          "parameters": "...",
          "order": 1,
          "reason": "..."
        },
        ...
      ],
      "response": "...",
      "reason": "..."
    },
    ...
  ]
}
```

API 의존성 문서를 생성하지 않으면 3번 스텝에서 에이전트가 잘못된 시나리오를 작성하거나 너무 쉬운 시나리오를 작성할 확률이 높아집니다. 의존성 문서 생성 시 그 이유를 같이 명시하도록 프롬프트를 작성하면 에이전트는 더 정확한 관계를 찾아냅니다.

#### 테스트 시나리오 문서

테스트 시나리오 문서는 테스트 코드를 작성할 때 필요한 시나리오(케이스)를 담습니다.

```
{
  "api_name": "Search_Documents",
  "scenarios": [
    {
      "name": "...",
      "description": "...",
      "preconditions": [
        {
          "api_name": "...",
          "purpose": "...",
          "endpoint": "...",
          "query_parameters": [],
          "request_body": { ... },
          "expected_response": { ... }
        }
      ],
      "request": { ... },
      "expected_response": { ... },
      "required_apis_for_verification": [ ... ],
      "side_effects": [ ... ],
      "verification": [ ... ],
      "cleanup": [ ... ]
    },
...
]
```

테스트 대상 API 호출 전 호출할 API(preconditions), 테스트 결과를 검증할 방법(verification), 테스트 후 발생한 사이드 이펙트를 원복시키는 과정(cleanup) 등을 포함합니다. 테스트가 만드는 사이드 이펙트와 테스트 검증 방법을 같이 명시하도록 프롬프트를 작성하면 에이전트는 더 정확한 시나리오를 작성하는 경향이 있습니다.

### 테스트 코드

테스트 코드 스텝은 코드를 작성하는 스텝(4번 스텝)과 검증하는 스텝(5번 스텝)으로 나뉩니다.

코드를 작성하는 스텝에서는 사용자 프롬프트가 중요합니다. 언어, 프레임워크, 애플리케이션 엔드포인트를 넘겨주는 방법, 오류 핸들링 방법 등을 최대한 구체적으로 적는 것이 좋습니다.

다음은 제가 작성한 프롬프트의 일부입니다.

```
Your task is to:  
1. Read and understand the API documentation from {api_docs_dir}  
2. Read and understand the test scenarios from {test_scenarios_dir}  
3. Read background knowledge if it exists.  
4. Generate comprehensive test code that covers all test scenarios  
5. Use environment variables for endpoints and configuration  
6. Follow Python testing best practices and patterns  
7. Retrieve the previous test code from the tool if it exists and refer it. You must be careful that the previous code is not always correct. Always prioritize the user API documentation. If incorrect information is included in the previous test code, you should modify it.

The generated test code should:  
- Use pytest framework
- Include proper setup and teardown methods
- Handle test data management
- Include proper assertions
- Use fixtures where appropriate
- Include proper error handling
- Follow PEP 8 style guidelines
```

테스트 코드를 검증하는 스텝에서는 JSON 형식 검증, 네이밍 컨벤션 검증, API 파라미터 이름 유효성 등을 검사합니다. 대다수의 테스트 코드 버그는 이러한 사소한 버그이므로 간단한 검증 과정이 테스트 코드의 품질 향상에 큰 도움이 됩니다.

테스트 코드를 실제로 실행해 디버깅하는 작업도 고려했었으나 추가하지 않았습니다. 검증 단계를 거치고도 버그가 발생하는 것은 문서가 충분하지 않아 LLM이 잘못 이해했거나 이전 단계에서 결과물이 잘못 만들어진 경우가 많기 때문입니다. 이러한 경우는 도메인 지식을 갖춘 개발자의 개입이 필요해 LLM이 스스로 디버깅하는 과정이 큰 도움을 주지 못합니다.

적용 결과
-----

MAFT는 짧은 시간인 3개월간 개발되었으나, 테스트 공백이 있었던 사내 검색 엔진 Noir의 6개 API에 대해 60개 이상의 테스트 케이스를 생성했고 15개 이상의 버그를 발견했습니다.

해당 API에 테스트 공백이 있었던 이유는 의존성이 있는 API가 많아 테스트 케이스를 작성하기에 너무 복잡했기 때문입니다. 그래서 테스트 코드를 작성하는 대신 개발자가 수동으로 테스트하는 경우가 대부분이었습니다. 이 때문에 API 공개 후 개발 과정에서 다른 로직을 고치면서 모르는 사이에 API 동작이 변경되면 이를 감지하지 못하는 경우가 왕왕 있었습니다.

MAFT는 제가 평소에 확인하던 조건보다 훨씬 더 꼼꼼하고 많은 테스트 케이스를 작성했고 이를 코드로 남겨, API의 정상 동작을 보장하는 데 큰 도움을 주었습니다. 특히 개발자가 놓치기 쉬운 에지 케이스를 다수 생성해 많은 버그를 찾아낼 수 있었습니다.

* API 파라미터 조건, 파라미터 기본값, 응답의 상태 코드 등이 사용자 문서와 다르게 설정되어 있음
* 배치로 다수의 문서를 입력/수정하는 API에서 일부 문서만 존재하지 않으면 오류 발생
* 입력에 공백, 특수 문자 등이 섞여 있으면 오류 발생
* 일부 케이스에 대해 API의 응답 형식이 다름
* API를 장시간 호출 시 오류 발생

MAFT가 가져온 또 다른 효과는 E2E 테스트 코드의 품질이 향상되었다는 것입니다. 그 중 하나로, 테스트 케이스가 다른 테스트 케이스에 영향을 주는 경우가 없어졌습니다. 기존 테스트 코드에서는 API가 애플리케이션의 상태를 변경해도(사이드 이펙트) 이를 원복하지 않아 다른 테스트 케이스에 영향을 미치는 경우가 간혹 있었습니다. MAFT는 테스트 코드에 상태를 원복하는 로직을 반드시 넣도록 해 각 테스트 케이스가 다른 테스트 케이스에 영향을 미치지 않고, 여러 번 호출해도 문제가 없습니다.

MAFT를 개발하면서 얻은 경험도 Noir 개발에 큰 도움이 되었습니다.

* MAFT에 제공하는 애플리케이션 API 문서는 과도하게 길지 않고 형식이 일관되었을 때 MAFT의 이해도가 높았습니다. 애플리케이션 API 문서를 간단하게 재작성하는 것만으로도 테스트의 품질이 크게 올라갑니다.
* 애플리케이션 API 문서 외 엔드포인트와 파라미터를 정리한 간단한 cheat sheet를 함께 제공하면 테스트 코드의 정확도가 유의미하게 향상되었습니다. LLM은 생각보다 엔드포인트와 파라미터를 문서에서 정확하게 뽑아내지 못합니다.
* LLM 모델은 품질과 버그 개수에 직접적인 영향을 주었습니다. GPT-4o를 사용했을 때와 다음 모델인 GPT-4.1을 사용했을 때 테스트의 버그 개수가 유의미하게 차이 났습니다.

MAFT 개발은 사용자 API 문서 재작성 및 cheat sheet 제작의 계기가 되었습니다. 아이러니하게도, LLM을 더 잘 이해시키려 하니 사용자 문서를 더 잘 작성하게 되었습니다.

한계점과 극복 방안
----------

MAFT는 목표했던 좋은 품질의 테스트를 만들어 내는 것에 성공했으나 몇 가지 한계점으로 인해 기존 CI 파이프라인으로의 통합은 보류했습니다.

* 비용이 많이 듦
* 생성된 테스트 코드 간 형식이 일관되지 않음
* 개발자의 검증이 여전히 필요함

가장 큰 단점은 비용입니다. LLM은 많은 일을 한 번에 하지 못하기 때문에 한 번에 여러 API의 테스트를 만들지 못합니다. MAFT는 스텝들을 API마다 반복하므로 테스트 대상 API 개수와 비용이 비례합니다. 테스트가 생성되는 시간은 API 1개당 약 10분이며 비용은 $1 정도입니다. Noir의 경우 6개의 API에 대한 테스트를 만드는 데 1시간이 걸리고 약 $7 정도의 비용이 발생했는데, 자주 호출하기에는 부담스러운 비용입니다.

API마다 스텝을 따로 수행해 생성된 테스트 코드 간 형식이 일관되지 않는 것은 코드 관리를 어렵게 합니다. 만약 테스트 코드를 프로젝트 일부로서 관리하려면 개발자가 쉽게 유지 보수할 수 있어야 합니다. 코드가 약속된 패턴을 사용하고 공통된 함수를 사용해야 개발자가 관리하기 쉬워지는데, MAFT는 기존 테스트 코드를 참고하지 않아 생성된 코드가 일관되지 않습니다.

마지막으로 MAFT로 생성된 테스트도 개발자의 검증이 여전히 필요합니다. 제 경험을 말하면, 10개의 테스트 케이스 검증에 약 1시간 정도가 소요되었습니다. 물론 처음부터 만드는 것보다는 시간이 훨씬 절약되지만, 수십 개의 API에 대해 테스트를 만든다면 여전히 부담스러울 수 있습니다.

하지만 위 한계점은 충분히 극복할 수 있습니다.

MAFT는 "모든 애플리케이션을 위한 E2E 테스트 생성 파이프라인"을 지향했기에 에이전트의 프롬프트에 특정 애플리케이션에 대한 정보가 없습니다. 만약 "Noir 특화 파이프라인"으로 목표를 변경해 에이전트의 프롬프트에 Noir에 대한 설명을 추가한다면 입력 시 필요한 문서의 양과 스텝 수를 줄일 수 있고, 이는 비용 감소로 이어질 것입니다.

테스트 코드 간 형식이 일관되지 않는 문제는 에이전트가 코드 작성 시 기존 테스트 코드를 참고하도록 하면 해결됩니다. 더 나아가 기존 테스트 코드를 분석하는 스텝을 추가한다면 훨씬 더 좋은 품질의 테스트 코드가 나올 수 있을 것입니다.

두 번째 한계점을 극복해 좋은 품질의 테스트 코드가 생성되면 마지막 한계점도 자연스럽게 극복할 수 있습니다. 생성한 테스트 코드를 검토할 때 가장 많이 걸렸던 부분은 바로 기존 테스트 코드와의 통합(중복된 코드 제거 등)이었기 때문입니다.

마치며
---

MAFT는 비용 문제와 생성된 코드의 형식 일관성 문제로 인해 기존 CI/CD 파이프라인으로의 통합이 보류되었습니다.

하지만 이 한계점은 Noir 특화 파이프라인과 같이 범용성 대신 도메인 특화를 택하면 극복할 수 있습니다. 또한 LLM의 비용 대비 성능이 빠르게 개선되고 있는 점을 고려하면, 가장 큰 단점인 MAFT의 비용 문제는 시간이 지날수록 완화될 가능성이 큽니다.

MAFT의 다음 목표는 Noir와 또 다른 검색 엔진인 [Dot](https://deview.kr/2016/schedule#session/135) 특화 E2E 테스트 생성 파이프라인으로 발전하는 것입니다. Noir와 Dot에 대한 사전 지식을 프롬프트에 주입함으로써 필요한 문서의 양을 줄이고 일부 스텝을 제거해 비용을 낮추고, 작업을 세밀하게 조정해 테스트 코드의 품질을 높일 계획입니다.

향후 과제를 완료해 MAFT가 통합에 성공한다면 또 다른 글로 찾아뵙겠습니다. 감사합니다.
