# Video Friday: This Drone Drives and Flies—Seamlessly

**출처:** [IEEE_Spectrum_Robotics](https://spectrum.ieee.org/video-friday-multimode-drone)

## 요약
![](https://spectrum.ieee.org/media-library/person-standing-near-a-black-drone-ready-for-flight-takeoff-on-a-concrete-floor-in-a-workshop.png?id=62089048&width=1245&height=700&coordinates=0%2C0%2C0%2C0)  
  

Video Friday is your weekly selection of awesome robotics videos, collected by your friends at *IEEE Spectrum* robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please [send us your events](mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday) for inclusion.

##### [ICRA 2026](https://2026.ieee-icra.org/): 1–5 June 2026, VIENNA

Enjoy today’s videos!

> *Unlike existing hybrid designs, Duawlfin eliminates the need for additional actuators or propeller-driven ground propulsion by leveraging only its standard quadrotor motors and introducing a differential drivetrain with one-way bearings. The seamless transitions between aerial and ground modes further underscore the practicality and effectiveness of our approach for applications like urban logistics and indoor navigation.*

[ [HiPeR Lab](https://sites.google.com/view/duawlfin) ]

I appreciate the softness of NEO’s design, but those fingers look awfully fragile.

[ [1X](https://www.1x.tech/neo) ]

> *Imagine reaching into your backpack to find your keys. Your eyes guide your hand to the opening, but once inside, you rely almost entirely on touch to distinguish your keys from your wallet, phone, and other items. This seamless transition between sensory modalities (knowing when to rely on vision versus touch) is something humans do effortlessly but robots struggle with. The challenge isn’t just about having multiple sensors. Modern robots are equipped with cameras, tactile sensors, depth sensors, and more. The real problem is \*\*how to integrate these different sensory streams\*\*, especially when some sensors provide sparse but critical information at key moments. Our solution comes from rethinking how we combine modalities. Instead of forcing all sensors through a single network, we train separate expert policies for each modality and learn how to combine their action predictions at the policy level.*

Multi-university Collaboration presented via [ [GitHub](https://policyconsensus.github.io/) ]

Thanks, Haonan!

Happy (somewhat late) Halloween from Pollen Robotics!

[ [Pollen Robotics](https://www.pollen-robotics.com/) ]

> *In collaboration with our colleagues from Iowa State and University of Georgia, we have put our pipe-crawling [worm robot](https://spectrum.ieee.org/underground-power-lines-robots) to test in the field. See it crawls through corrugated drainage pipes in a stream, and a smooth section of a subsurface drainage system.*

[ [Paper](https://ieeexplore.ieee.org/document/11018750) ] from [ [Smart Microsystems Laboratory, Michigan State University](https://smlab.msu.edu/) ]

> *Heterogeneous robot teams operating in realistic settings often must accomplish complex missions requiring collaboration and adaptation to information acquired online. Because robot teams frequently operate in unstructured environments — uncertain, open-world settings without prior maps — subtasks must be grounded in robot capabilities and the physical world. We present SPINE-HT, a framework that addresses these limitations by grounding the reasoning abilities of LLMs in the context of a [heterogeneous robot team](https://spectrum.ieee.org/iros-2012-robotic-airplane-boat-and-submarine-team-up-to-monitor-coral-reefs) through a three-stage process. In real-world experiments with a Clearpath Jackal, a Clearpath Husky, a Boston Dynamics Spot, and a high-altitude UAV, our method achieves an 87% success rate in missions requiring reasoning about robot capabilities and refining subtasks with online feedback.*

[ [SPINE-HT](https://zacravichandran.github.io/SPINE-HT/) ] from [ [GRASP Lab, University of Pennsylvania](https://www.grasp.upenn.edu/) ]

Astribot keeping itself busy at IROS 2025.

[ [Astribot](https://www.astribot.com/en) ]

> *In two papers published in* Matter *and Advanced Science, a team of scientists from the Physical Intelligence Department at the Max Planck Institute for Intelligent Systems in Stuttgart, Germany, developed control strategies for influencing the motion of self-propelling oil droplets. These oil droplets mimic single-celled microorganisms and can autonomously solve a complex maze by following chemical gradients. However, it is very challenging to integrate external perturbation and use these droplets in robotics. To address these challenges, the team developed magnetic droplets that still possess life-like properties and can be controlled by external magnetic fields. In their work, the researchers showed that they are able to guide the droplet’s motion and use them in microrobotic applications such as cargo transportation.*

[ [Max Planck Institute](https://is.mpg.de/pi/news/blog-active-droplets-get-a-magnetic-upgrade-controlling-self-propelling-droplets-and-their-collectives) ]

> *Everyone has fantasized about having an embodied avatar! Full-body teleoperation and full-body data acquisition platform is waiting for you to try it out!*

[ [Unitree](https://www.unitree.com/g1) ]

It’s not a [humanoid](https://spectrum.ieee.org/humanoid-robot-olympics), but it right now safely does useful things and probably doesn’t cost all that much to buy or run.

[ [Naver Labs](https://www.naverlabs.com/en/storyDetail/343) ]

> *This paper presents a curriculum-based reinforcement learning framework for training precise and high-performance jumping policies for the robot `Olympus’. Separate policies are developed for vertical and horizontal jumps, leveraging a simple yet effective strategy. Experimental validation demonstrates horizontal jumps up to 1.25 m with centimeter accuracy and vertical jumps up to 1.0 m. Additionally, we show that with only minor modifications, the proposed method can be used to learn omnidirectional jumping.*

[ [Paper](https://arxiv.org/abs/2510.24584) ] from [ [Autonomous Robots Lab, Norwegian University of Science and Technology](https://www.autonomousrobotslab.com/) ]

> *Heavy payloads are no problem for it: The new KR TITAN ultra moves payloads of up to 1500 kg, making the heavy lifting extreme in the KUKA portfolio.*

[ [Kuka](https://www.kuka.com/kr-titan-ultra) ]

Good luck getting all of the sand out of that robot. Perhaps a nice oil bath is in order?

[ [DEEP Robotics](https://www.deeprobotics.cn/en) ]

This CMU RI Seminar is from Yuke Zhu at University of Texas at Austin, on “Toward Generalist Humanoid Robots: Recent Advances, Opportunities, and Challenges.”

> *In an era of rapid AI progress, leveraging accelerated computing and big data has unlocked new possibilities to develop generalist AI models. As AI systems like ChatGPT showcase remarkable performance in the digital realm, we are compelled to ask: Can we achieve similar breakthroughs in the physical world — to create generalist humanoid robots capable of performing everyday tasks? In this talk, I will outline our data-centric research principles and approaches for building general-purpose robot autonomy in the open world. I will present our recent work leveraging real-world, synthetic, and web data to train foundation models for humanoid robots. Furthermore, I will discuss the opportunities and challenges of building the next generation of intelligent robots.*

[ [Carnegie Mellon University Robotics Institute](https://www.ri.cmu.edu/event/toward-generalist-humanoid-robots-recent-advances-opportunities-and-challenges/) ]
