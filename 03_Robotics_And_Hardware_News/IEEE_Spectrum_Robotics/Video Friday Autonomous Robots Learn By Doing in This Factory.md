# Video Friday: Autonomous Robots Learn By Doing in This Factory

**출처:** [IEEE_Spectrum_Robotics](https://spectrum.ieee.org/autonomous-warehouse-robots)

## 요약
![](https://spectrum.ieee.org/media-library/robotic-arms-on-mobile-bases-sort-crates-on-a-conveyor-belt-in-a-warehouse.png?id=63907821&width=1200&height=400&coordinates=0%2C256%2C0%2C256)  
  

Video Friday is your weekly selection of awesome robotics videos, collected by your friends at *IEEE Spectrum* robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please [send us your events](mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday) for inclusion.

##### [ICRA 2026](https://2026.ieee-icra.org/): 1–5 June 2026, VIENNA

Enjoy today’s videos!

> *To train the next generation of [autonomous robots](https://spectrum.ieee.org/toyota-to-invest-1-billion-in-ai-and-robotics-rd), scientists at Toyota Research Institute are working with Toyota Manufacturing to deploy them on the factory floor.*

[ [Toyota Research Institute](https://www.linkedin.com/posts/toyota-research-institute_whats-next-for-tri-robotics-max-bajracharya-activity-7424198589196685313-4e92?utm_source=share&utm_medium=member_desktop&rcm=ACoAAAM4nT0BW_DvaXaoyr7IuJL-to9SJ5MlYT4) ]

Thanks, Erin!

> *This is just one story (of many) about how we tried, failed, and learned how to improve our ‪[drone delivery](https://spectrum.ieee.org/in-the-air-with-ziplines-medical-delivery-drones) system.*

Okay, but like you didn’t show the really cool bit...?

[ [Zipline](https://www.zipline.com/) ]

> *We’re introducing KinetIQ, an AI framework developed by Humanoid, for end-to-end orchestration of humanoid robot fleets. KinetIQ coordinates wheeled and bipedal robots within a single system, managing both fleet-level operations and individual robot behavior across multiple environments. The framework operates across four cognitive layers, from task allocation and workflow optimization to task execution based on Vision-Language-Action models and whole-body control taught by reinforcement learning, and is shown here running across our wheeled industrial robots and bipedal R&D platform.*

[ [Humanoid](https://thehumanoid.ai/) ]

> *What if a robot gets damaged during operation? Can it still perform its mission without immediate repair? Inspired by the self-embodied resilience strategies of stick insects, we developed a decentralized adaptive resilient neural control system (DARCON). This system allows legged robots to autonomously adapt to limb loss, ensuring mission success despite mechanical failure. This innovative approach leads to a future of truly resilient, self-recovering robotics.*

[ [VISTEC](https://advanced.onlinelibrary.wiley.com/doi/10.1002/aisy.202500270) ]

Thanks, Poramate!

> *This animation shows Perseverance’s point of view during a drive of 807 feet (246 meters) along the rim of Jezero Crater on 10 December 2025, the 1,709th Martian day, or sol, of the mission. Captured over 2 hours and 35 minutes, 53 navigation-camera (Navcam) image pairs were combined with rover data on orientation, wheel speed, and steering angle, as well as data from Perseverance’s inertial measurement unit, and placed into a 3D virtual environment. The result is this reconstruction with virtual frames inserted about every 4 inches (0.1 meters) of drive progress.*

[ [NASA Jet Propulsion Lab](https://science.nasa.gov/mission/mars-2020-perseverance/) ]

> *−47.4 °C, 130,000 steps, 89.75°E, 47.21°N… On the extremely cold snowfields of Altay, the birthplace of human skiing, Unitree’s humanoid robot G1 left behind a unique set of marks.*

[ [Unitree](https://www.unitree.com/) ]

> *Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a vision language model (VLM) to infer semantic relationships. Notably, we introduce a task-reasoning module that combines large language models and a VLM to interpret the scene graph’s semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.*

[ [Norwegian University of Science & Technology, Autonomous Robots Lab](https://ntnu-arl.github.io/reasoning_graph/) ]

Thanks, Kostas!

> *We present HoLoArm, a quadrotor with compliant arms inspired by the nodus structure of dragonfly wings. This design provides natural flexibility and resilience while preserving flight stability, which is further reinforced by the integration of a reinforcement-learning control policy that enhances both recovery and hovering performance.*

[ [HO Lab via IEEE Robotics and Automation Letters](https://ieeexplore.ieee.org/abstract/document/11361075) ]

> *In this work, we present SkyDreamer, to the best of our knowledge the first end-to-end vision-based autonomous-drone racing policy that maps directly from pixel-level representations to motor commands.*

[ [MAVLab](https://arxiv.org/pdf/2510.14783) ]

> *This video showcases AI Worker, equipped with five-finger hands, performing dexterous object manipulation across diverse environments. Through teleoperation, the robot demonstrates precise, humanlike hand control in a variety of manipulation tasks.*

[ [Robotis](https://ai.robotis.com/hands/introduction_hands.html) ]

> *Autonomous following, 45-degree slope climbing, and reliable payload transport in extreme winter conditions, built to support operations where environments push the limits.*

[ [DEEP Robotics](https://www.deeprobotics.cn/en) ]

> *Living architectures, from plants to beehives, adapt continuously to their environments through self-organization. In this work, we introduce the concept of architectural swarms: systems that integrate swarm robotics into modular architectural façades. The Swarm Garden exemplifies how architectural swarms can transform the built environment, enabling “living-like” architecture for functional and creative applications.*

[ [SSR Lab via Science Robotics](https://www.science.org/doi/10.1126/scirobotics.ady7233) ]

Here are a couple of IROS 2025 keynotes, featuring Bram Vanderborght and Kyu-Jin Cho.

- YouTube  [www.youtube.com](https://www.youtube.com/watch?v=j6fEnhU56aA)

[ [IROS 2025](https://www.iros25.org/) ]
