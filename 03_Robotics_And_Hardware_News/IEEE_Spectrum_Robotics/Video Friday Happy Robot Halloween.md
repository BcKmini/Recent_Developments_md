# Video Friday: Happy Robot Halloween!

**출처:** [IEEE_Spectrum_Robotics](https://spectrum.ieee.org/video-friday-robot-halloween-2674252642)

## 요약
![](https://spectrum.ieee.org/media-library/robot-arm-holding-a-creepy-mummy-head-with-round-eyes-lit-in-eerie-blue-light.png?id=61993237&width=1200&height=800&coordinates=150%2C0%2C150%2C0)  
  

Video Friday is your weekly selection of awesome robotics videos, collected by your friends at *IEEE Spectrum* robotics. We also post a weekly calendar of upcoming robotics events for the next few months. Please [send us your events](mailto:automaton@ieee.org?subject=Robotics%20event%20suggestion%20for%20Video%20Friday) for inclusion.

##### [ICRA 2026](https://2026.ieee-icra.org/): 1–5 June 2026, VIENNA

Enjoy today’s videos!

Happy Halloween from UCL!

[ [University College London](https://rpl-as-ucl.github.io/) ]

Happy Halloween from [KIMLAB](https://spectrum.ieee.org/video-friday-lassie-moon)!

[ [Kinetic Intelligent Machine Lab](https://publish.illinois.edu/kimlab2020/) ]

Happy Halloween from the [DRAGON Lab](https://spectrum.ieee.org/flying-dragon-robot-transforms-itself-to-squeeze-through-gaps)!

[ [DRAGON Lab, University of Tokyo](https://www.dragon.t.u-tokyo.ac.jp/) ]

Thanks, Moju!

Happy Halloween from [Agility Robotics](https://spectrum.ieee.org/agility-robotics-digit-v2-biped-robot)!

[ [Agility Robotics](https://www.agilityrobotics.com/) ]

Happy Halloween from HEBI Robotics!

[ [HEBI Robotics](https://www.hebirobotics.com/) ]

You can now pay 1X $500/mo to collect data in your home.

And it’s about what you’d expect:

[ [1X](https://www.1x.tech/neo) ] via [ [WSJ](https://www.wsj.com/tech/personal-tech/i-tried-the-robot-thats-coming-to-live-with-you-its-still-part-human-68515d44) ]

> *At our test warehouse, we recreate our customers’ inbound operations, from the dock configuration and conveyors, to the freight and beyond. Step inside our Stretch testing facility to learn about the the latest developments in warehouse automation and explore how we ensure robust, reliable performance in the real world.*

[ [Boston Dynamics](https://bostondynamics.com/blog/inside-the-stretch-lab/) ]

Well this is just mean. Important, but mean.

[ [Istituto Italiano de Tecnologia](https://dls.iit.it/) ]

> *SpikeATac is a a multimodal tactile finger combining a taxelized and highly sensitive dynamic response (PVDF) with a static transduction method (capacitive) for multimodal touch sensing. Named for its `spiky’ response, SpikeATac’s multitaxel PVDF film provides fast, sensitive dynamic signals to the very onset and breaking of contact, providing the ability to stop quickly and delicately when grasping fragile, deformable objects.*

[ [ROAM Lab, Columbia University](https://roamlab.github.io/spikeatac/) ]

> *Effectively integrating diverse sensory representations is crucial for robust robotic manipulation. However, the typical approach of feature concatenation is often suboptimal: dominant modalities such as vision can overwhelm sparse but critical signals like touch in contact-rich tasks, and monolithic architectures cannot flexibly incorporate new or missing modalities without retraining. Our method factorizes the policy into a set of diffusion models, each specialized for a single representation (e.g., vision or touch), and employs a router network that learns consensus weights to adaptively combine their contributions, enabling incremental integration of new representations.*

[ [GitHub](https://policyconsensus.github.io/) ]

Thanks, Haonan!

> *General-purpose robots should possess human-like dexterity and agility to perform tasks with the same versatility as us. A human-like form factor further enables the use of vast datasets of human-hand interactions. However, the primary bottleneck in dexterous manipulation lies not only in software but arguably even more in hardware. We present the open-source ORCA hand, a reliable and anthropomorphic 17-DoF tendon-driven robotic hand with integrated tactile sensors, fully assembled in less than eight hours and built for a material cost below 2,000 CHF.*

[ [ORCA](https://orca.ethz.ch/) ]

> *University of Chicago computer scientist Sarah Sebo is programming robots to give empathetic responses and perform nonverbal social cues like nodding to better build trust and rapport with humans. The goal is to develop robots that can improve performance in [human-robot teams](https://spectrum.ieee.org/the-buddy-system-human-computer-teams), such as enhancing learning outcomes for children.*

[ [University of Chicago](https://news.uchicago.edu/story/sebo-lab-programming-robots-better-interact-humans) ]

[DJI](https://spectrum.ieee.org/the-consumer-electronics-hall-of-fame-dji-phantom-drone) has a [robot vacuum](https://spectrum.ieee.org/you-definitely-need-a-robotic-spy-vacuum) now, which is fine. As far as I can make out, we’ve reached the point where just about every robot vacuum is (for better or worse) just that: fine.

[ [DJI](https://www.romo.tech/) ]

This ICRA 2025 keynote is from Angela Schoellig at Technical University of Munich, on “Powering Robotics with AI.”

[ [ICRA 2025](https://2025.ieee-icra.org/program/keynote-sessions/) ]

This Carnegie Mellon University, Robotics Institute (CMU RI) Seminar is from Nancy Pollard, on “Bringing Dexterity to Robot Hands in the Real World.”

> *Dexterous manipulation is a grand challenge of robotics, and fine manipulation skills are required for many robotics applications that we envision. In this overview talk, I will discuss my view of some major factors that contribute to dexterity and discuss how we can incorporate them into our robots and systems.*

[ [CMU RI](https://www.ri.cmu.edu/event/bringing-dexterity-to-robot-hands-in-the-real-world/) ]
